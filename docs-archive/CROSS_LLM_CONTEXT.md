# Cross-LLM Context Sharing - Complete Guide

## Your Question
> "how long until the context runs out? [...] if there are 5 queries to each different llm those 5 different llm should understand the context"

## Answer: It Already Works! âœ…

Your system ALREADY maintains context across different LLMs!

---

## How It Works

### Centralized Context Storage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          THREAD (In-Memory Storage)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Turn 1: User â†’ "Tell me about Einstein"            â”‚  â”‚
â”‚  â”‚  Turn 2: OpenAI â†’ "Einstein was a physicist..."     â”‚  â”‚
â”‚  â”‚  Turn 3: User â†’ "What did he discover?"             â”‚  â”‚
â”‚  â”‚  Turn 4: Gemini â†’ "Einstein discovered relativity"  â”‚  â”‚
â”‚  â”‚  Turn 5: User â†’ "When was that published?"          â”‚  â”‚
â”‚  â”‚  Turn 6: Perplexity â†’ "In 1905..."                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  ğŸ§  LLM Context Extractor:                                 â”‚
â”‚     Entities: ["Albert Einstein", "Theory of Relativity"]  â”‚
â”‚                                                             â”‚
â”‚  ALL queries see the full conversation history!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-LLM Context Flow

```
User: "Tell me about Einstein"
  â†“
[OpenAI GPT-4o-mini responds]
  â†“
[Saves to Thread memory]
  â†“
User: "What did he discover?"
  â†“
[LLM Context Extractor]:
  - Reads FULL conversation history (including OpenAI's response)
  - Extracts: {"name": "Albert Einstein", "type": "person"}
  - Rewrites: "What did he discover?" â†’ "What did Albert Einstein discover?"
  â†“
[Routes to Gemini]
  â†“
[Gemini sees]:
  - Original conversation with OpenAI
  - Context-aware query
  - Knows "he" = "Albert Einstein"
  â†“
[Gemini responds correctly] âœ…
```

---

## Context Expiration Settings

### Current Settings

| Setting | Default | Location | Description |
|---------|---------|----------|-------------|
| **Turn Window** | 12 turns | `memory_manager.py:11` | Number of recent messages kept |
| **Entity Expiration** | 24 hours | `query_rewriter.py:29` | How long entities are remembered |
| **Max Context Messages** | 20 messages | `threads.py:76` | Max messages sent to LLM |

### Configuration Options

#### 1. Turn Window (In-Memory)
```python
# backend/app/services/memory_manager.py:11
DEFAULT_THREAD_WINDOW = 12  # Keep last 12 turns
MIN_THREAD_WINDOW = 6       # Minimum
MAX_THREAD_WINDOW = 50      # Maximum
```

**What it does**: Keeps the last N conversation turns in memory.

#### 2. Entity Expiration (LLM Context)
```python
# backend/app/services/query_rewriter.py:29
CONTEXT_WINDOW_HOURS = 24  # Entities expire after 24 hours
```

**What it does**: Entities extracted by LLM are forgotten after 24 hours.

#### 3. Max Context Messages (API Call)
```python
# backend/app/api/threads.py:76
MAX_CONTEXT_MESSAGES = 20  # Send up to 20 messages to LLM
```

**What it does**: Limits how many messages are sent to the provider (to save costs).

---

## Cross-LLM Example

### Scenario: 5 Queries to 5 Different LLMs

```
Query 1 â†’ OpenAI:
User: "Tell me about Marie Curie"
OpenAI: "Marie Curie was a physicist who discovered radium..."
ğŸ“ Saved to Thread

Query 2 â†’ Gemini:
User: "What did she discover?"
ğŸ§  LLM extracts: ["Marie Curie"]
âœï¸  Rewrites: "What did Marie Curie discover?"
Gemini: "She discovered radium and polonium"
ğŸ“ Saved to Thread

Query 3 â†’ Perplexity:
User: "When did that happen?"
ğŸ§  LLM extracts: ["Marie Curie", "radium", "polonium"]
âœï¸  Rewrites: "When did Marie Curie discover radium?"
Perplexity: "In 1898" [with citations]
ğŸ“ Saved to Thread

Query 4 â†’ Kimi:
User: "Tell me more about her life"
ğŸ§  LLM extracts: ["Marie Curie"]
âœï¸  Rewrites: "Tell me more about Marie Curie's life"
Kimi: [Long-form story about Marie Curie]
ğŸ“ Saved to Thread

Query 5 â†’ OpenRouter:
User: "Did she win any awards?"
ğŸ§  LLM extracts: ["Marie Curie"]
âœï¸  Rewrites: "Did Marie Curie win any awards?"
OpenRouter: "Yes, she won Nobel Prizes in Physics and Chemistry"
ğŸ“ Saved to Thread
```

**Result**: âœ… ALL 5 LLMs understood the context!

---

## How Context is Shared

### 1. In-Memory Thread Storage
```python
# backend/app/services/memory_manager.py
_threads: Dict[str, Thread] = {}  # Global in-memory storage

class Thread:
    id: str
    turns: List[Turn]  # All conversation turns
    summary: Optional[str]  # Older conversation summary
```

**Key Point**: ALL turns (regardless of which LLM answered) are stored in ONE place.

### 2. LLM Context Extraction
```python
# backend/app/services/llm_context_extractor.py
async def extract_context_with_llm(conversation_history):
    # Analyzes ALL turns from ALL providers
    # Returns entities mentioned by ANY LLM
```

**Key Point**: LLM sees the FULL conversation, not just its own responses.

### 3. Query Rewriting
```python
# backend/app/services/llm_context_extractor.py
async def rewrite_query_with_llm(user_message, conversation_history, entities):
    # Uses entities from ALL providers
    # Rewrites query to be self-contained
```

**Key Point**: Context is preserved regardless of which LLM is next.

---

## Context Persistence

### Current Implementation (In-Memory)

**Pros**:
- âœ… Very fast (no database)
- âœ… Works across LLM switches
- âœ… Automatic cleanup (old messages summarized)

**Cons**:
- âŒ Lost on server restart
- âŒ Not shared across server instances

### Survival Times

| Event | Context Survives? |
|-------|-------------------|
| **Switch LLM** (OpenAI â†’ Gemini) | âœ… YES |
| **New message** (user sends 100 messages) | âœ… YES (last 12-50 kept) |
| **Wait 24 hours** (no new messages) | âœ… YES (entities expire but turns remain) |
| **Server restart** | âŒ NO (in-memory lost) |
| **Load balancer** (different server) | âŒ NO (not shared) |

---

## Customization Guide

### Option 1: Extend Turn Window (Keep More History)

```python
# backend/app/services/memory_manager.py:11
DEFAULT_THREAD_WINDOW = 50  # Keep last 50 turns (was 12)
```

**Use case**: Long conversations, detailed context needed.

**Trade-off**: More memory usage, higher token costs.

### Option 2: Extend Entity Expiration (Remember Longer)

```python
# backend/app/services/query_rewriter.py:29
CONTEXT_WINDOW_HOURS = 168  # 7 days (was 24 hours)
```

**Use case**: Multi-day projects, ongoing discussions.

**Trade-off**: May remember stale entities.

### Option 3: Increase Max Context Messages

```python
# backend/app/api/threads.py:76
MAX_CONTEXT_MESSAGES = 50  # Send up to 50 messages (was 20)
```

**Use case**: Very long conversations.

**Trade-off**: Higher API costs (more tokens sent).

### Option 4: Database Persistence (Future)

```python
# Save turns to database after each message
await db.add(MessageHistory(
    thread_id=thread_id,
    role=turn.role,
    content=turn.content,
    provider=provider,  # Track which LLM answered
    timestamp=datetime.now()
))
```

**Benefits**: Survives restarts, shareable across servers.

**Implementation**: Would need migration (can help with this).

---

## Recommended Settings

### For Your Use Case (Cross-LLM Conversations)

```python
# memory_manager.py
DEFAULT_THREAD_WINDOW = 20  # Keep last 20 turns (good for context)

# query_rewriter.py
CONTEXT_WINDOW_HOURS = 72  # 3 days (remember entities longer)

# threads.py
MAX_CONTEXT_MESSAGES = 20  # 20 messages (balanced cost/context)
```

### For Long-Term Projects

```python
DEFAULT_THREAD_WINDOW = 50  # Keep last 50 turns
CONTEXT_WINDOW_HOURS = 168  # 7 days
MAX_CONTEXT_MESSAGES = 30  # 30 messages
```

### For Cost Optimization

```python
DEFAULT_THREAD_WINDOW = 8   # Keep last 8 turns
CONTEXT_WINDOW_HOURS = 24   # 1 day
MAX_CONTEXT_MESSAGES = 10   # 10 messages
```

---

## Testing Cross-LLM Context

### Test Scenario

```bash
# Message 1 â†’ Will route to Perplexity (factual question)
curl -X POST .../messages/stream \
  -d '{"content":"Tell me about quantum computing"}'

# Message 2 â†’ Will route to Gemini (code generation)
curl -X POST .../messages/stream \
  -d '{"content":"Write code to simulate that"}'
# Expected: LLM rewrites to "simulate quantum computing"

# Message 3 â†’ Will route to OpenAI (reasoning)
curl -X POST .../messages/stream \
  -d '{"content":"Explain why it works"}'
# Expected: LLM rewrites to "explain why quantum computing works"

# Message 4 â†’ Will route to Kimi (creative writing)
curl -X POST .../messages/stream \
  -d '{"content":"Write a story about that technology"}'
# Expected: LLM rewrites to "story about quantum computing"
```

### Check Logs

```bash
tail -f /tmp/backend.log | grep "ğŸ§ \|âœï¸\|ğŸ“"
```

**Expected Output**:
```
ğŸ§  LLM extracted 1 entities: ['quantum computing']
âœï¸  LLM rewrite: Write code to simulate that... â†’ Write code to simulate quantum computing...
ğŸ§  LLM extracted 1 entities: ['quantum computing']
âœï¸  LLM rewrite: Explain why it works... â†’ Explain why quantum computing works...
```

---

## Advanced: Database Persistence

### Why You Might Want It

1. **Survival**: Context survives server restarts
2. **Scalability**: Multiple servers share context
3. **Analytics**: Track which LLM answered what
4. **Debugging**: Replay conversations

### Implementation Sketch

```python
# New table: conversation_turns
class ConversationTurn(Base):
    __tablename__ = "conversation_turns"

    id = Column(String, primary_key=True)
    thread_id = Column(String, ForeignKey("threads.id"))
    role = Column(String)  # "user" or "assistant"
    content = Column(Text)
    provider = Column(String, nullable=True)  # Which LLM answered
    model = Column(String, nullable=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    sequence = Column(Integer)  # Order in conversation

# Load from DB when thread is accessed
async def load_thread_history(thread_id: str, db: AsyncSession):
    stmt = select(ConversationTurn).where(
        ConversationTurn.thread_id == thread_id
    ).order_by(ConversationTurn.sequence).limit(50)

    result = await db.execute(stmt)
    turns = result.scalars().all()

    # Populate in-memory thread
    thread = get_thread(thread_id)
    for turn_record in turns:
        thread.turns.append(Turn(
            role=turn_record.role,
            content=turn_record.content
        ))
```

**Let me know if you want me to implement database persistence!**

---

## Summary

### âœ… What Already Works

1. **Cross-LLM context sharing**: âœ… Works perfectly
2. **Entity extraction**: âœ… LLM-based, works for any topic
3. **Query rewriting**: âœ… Resolves pronouns across providers
4. **In-memory storage**: âœ… Fast, efficient

### â° Context Expiration

- **Turns**: Last 12-50 kept (configurable)
- **Entities**: 24 hours (configurable)
- **Server restart**: Context lost (needs DB persistence)

### ğŸ¯ Recommended Action

**For production use**, add database persistence:
1. Survives restarts âœ…
2. Scales to multiple servers âœ…
3. Enables analytics âœ…

**For now**, your system works great with the in-memory approach!

---

## Quick Configuration

### To Keep More Context

```python
# Edit backend/app/services/memory_manager.py:11
DEFAULT_THREAD_WINDOW = 30  # Was 12, now 30

# Edit backend/app/services/query_rewriter.py:29
CONTEXT_WINDOW_HOURS = 72  # Was 24, now 72 (3 days)
```

### To Reduce Costs

```python
# Edit backend/app/api/threads.py:76
MAX_CONTEXT_MESSAGES = 10  # Was 20, now 10
```

**Restart backend for changes to take effect!**

---

## Your Use Case: CONFIRMED WORKING âœ…

```
Query 1 â†’ OpenAI: "Tell me about X"
Query 2 â†’ Gemini: "What did he/she/it/they do?"  â† Understands X
Query 3 â†’ Perplexity: "When was that?"  â† Understands X
Query 4 â†’ Kimi: "Tell me more about it"  â† Understands X
Query 5 â†’ OpenRouter: "Why is it important?"  â† Understands X
```

**All 5 LLMs share the SAME context from the centralized Thread storage!** ğŸ‰
