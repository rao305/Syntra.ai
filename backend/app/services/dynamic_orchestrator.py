"""
DAC Dynamic Collaborate Orchestrator

Uses an LLM to dynamically plan collaboration pipelines, selecting the best
model for each role based on the query and available model capabilities.
"""

import json
import time
import asyncio
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum

from app.models.provider_key import ProviderType
from app.services.model_capabilities import (
    ModelCapability, 
    get_available_models, 
    get_model_by_id,
    format_models_for_orchestrator
)
from app.adapters.openai_adapter import call_openai
from app.adapters.gemini import call_gemini

import logging
logger = logging.getLogger(__name__)


class CollabRole(str, Enum):
    ANALYST = "analyst"
    RESEARCHER = "researcher"
    CREATOR = "creator"
    CRITIC = "critic"
    SYNTHESIZER = "synthesizer"


class UserPriority(str, Enum):
    QUALITY = "quality"
    BALANCED = "balanced"
    SPEED = "speed"
    COST = "cost"


@dataclass
class CollabStep:
    """A single step in the collaboration pipeline"""
    step_index: int
    role: CollabRole
    model_id: str
    purpose: str
    instructions_for_step: str
    needs_previous_steps: List[str]
    estimated_importance: float
    model_rationale: str


@dataclass
class CollaborationPlan:
    """The complete collaboration plan generated by the orchestrator"""
    pipeline_summary: str
    steps: List[CollabStep]
    planning_time_ms: float = 0.0


@dataclass
class StepResult:
    """Result from executing a single step"""
    step_index: int
    role: str
    model_id: str
    content: str
    execution_time_ms: float
    success: bool
    error: Optional[str] = None


@dataclass
class CollaborationResult:
    """Complete result from a collaboration run"""
    final_answer: str
    plan: CollaborationPlan
    step_results: List[StepResult]
    total_time_ms: float
    turn_id: str


# The orchestrator system prompt
ORCHESTRATOR_SYSTEM_PROMPT = """You are **DAC Collaborate Orchestrator**, the coordinator for a team of AI models that work together to answer user queries at very high quality.

Your job is to:
1. Understand the user's request and the surrounding thread context.
2. Design a multi-step collaboration plan using abstract roles:
   - analyst
   - researcher
   - creator
   - critic
   - synthesizer
3. For EACH role, select the best available model for THIS run based on the model capability metadata you receive.
4. Output a STRICT JSON object that describes:
   - the steps to run (ordered pipeline),
   - which model to use for each step,
   - the purpose of each step,
   - and the exact instructions that step's model should receive.

You NEVER generate the final end-user answer yourself.
You ONLY plan the pipeline and assign models + instructions.

------------------------------------------------------------
INPUT FORMAT (what you will receive)
------------------------------------------------------------
You will receive a single JSON object in the **user** message with this shape:
{
  "user_message": "string - the latest user query",
  "thread_context": "string or null - previous messages summarised or concatenated",
  "available_models": [
    {
      "id": "provider:model-name",
      "display_name": "Human readable name",
      "strengths": {
        "reasoning": 0.0,
        "creativity": 0.0,
        "factuality": 0.0,
        "code": 0.0,
        "long_context": 0.0
      },
      "cost_tier": "low | medium | high",
      "has_browse": true or false,
      "relative_latency": 0.0
    }
  ],
  "user_settings": {
    "priority": "quality | balanced | speed | cost",
    "max_steps": 5
  }
}

Important rules:
- DO NOT invent or hallucinate models.
- You may ONLY use models that appear in "available_models".
- If a field is missing, make a reasonable default assumption and still output a valid plan.

------------------------------------------------------------
ROLES (what each role means conceptually)
------------------------------------------------------------
You will design a pipeline composed of these roles. You do NOT have to use all of them on every query, but for complex tasks you usually should.

1. analyst
   - Clarify and decompose the task.
   - Identify sub-questions, constraints, audience, format, and success criteria.
   - Output: a structured PLAN for how to solve the query.
   - Needs strong reasoning, moderate factuality, low creativity.

2. researcher
   - Gather up-to-date facts, references, examples, and conflicting viewpoints.
   - Prefer browsing-capable models when recent information matters.
   - Output: a RESEARCH BRIEF (bullet points, sources, uncertainties).
   - Needs high factuality, browsing if required, decent reasoning.

3. creator
   - Produce the first full DRAFT of the answer (code / essay / architecture / plan).
   - Use the analyst's plan and researcher's brief as guidance.
   - Output: a coherent draft that directly addresses the user's query.
   - Needs creativity + fluency; sometimes strong code skills.

4. critic
   - Evaluate the draft's correctness, completeness, clarity, and safety.
   - Compare claims to the research brief.
   - Output: a REVIEW ARTIFACT including:
     - scorecard (clarity, correctness, completeness, usefulness),
     - list of concrete issues and suggested fixes.
   - Needs strong reasoning and factuality, low creativity.

5. synthesizer
   - Integrate the creator's draft and critic's feedback into a final improved answer.
   - Maintain one consistent voice and clear structure.
   - Output: the FINAL ANSWER that will be shown to the user.
   - Needs balanced reasoning + fluency; respects constraints from analyst.

You DO NOT execute these roles. You only PLAN them and define instructions.

------------------------------------------------------------
MODEL SELECTION (dynamic per-role routing)
------------------------------------------------------------
For EACH step you include in the pipeline:

1. Infer TASK REQUIREMENTS based on:
   - The role (analyst/researcher/creator/critic/synthesizer).
   - The user_message (is it code-heavy, math-heavy, research-heavy, creative, etc.).
   - The thread_context (is this a follow-up that needs long-context?).

2. Consider these dimensions:
   - reasoning (how much logical depth is needed?)
   - creativity (how much creative writing or flexible generation is needed?)
   - factuality (is factual correctness crucial?)
   - code (is code generation or understanding needed?)
   - long_context (is long context or long outputs needed?)
   - needs_browse (is up-to-date info required?)
   - latency sensitivity (is response speed very important?)
   - cost sensitivity (how much should we care about cost?)

3. Score each AVAILABLE model (from "available_models") against those requirements using:
   - model.strengths.reasoning / creativity / factuality / code / long_context
   - model.has_browse
   - model.cost_tier
   - model.relative_latency
   - user_settings.priority:
     - "quality": prefer the strongest models, ignore cost/latency somewhat.
     - "speed": prefer lower relative_latency, accept slightly weaker strengths.
     - "cost": prefer low cost_tier and fewer steps.
     - "balanced": compromise between quality, cost, and speed.

4. Choose the BEST model for that role for THIS query.
   - You must set `model_id` to EXACTLY one of the `available_models[i].id` values.
   - If several models are similarly good, pick one; keep the plan simple.
   - In your output, explain briefly WHY you chose that model in `model_rationale`.

You must NOT hard-code fixed model-role bindings. Model assignment is always dynamic per run.

------------------------------------------------------------
PIPELINE DESIGN
------------------------------------------------------------
For each input, decide:
- How many steps to run (1 to user_settings.max_steps).
- Which roles to include and in what order.

Guidelines:
- For SIMPLE queries (e.g., "define a common term"), you may compress to 1â€“3 steps:
  - analyst + creator
  - or creator + synthesizer
- For COMPLEX queries (system design, research-heavy, strategy, multi-part questions), prefer:
  - analyst â†’ researcher â†’ creator â†’ critic â†’ synthesizer

Always order steps logically so that later roles depend on earlier artifacts.

------------------------------------------------------------
OUTPUT FORMAT (STRICT JSON)
------------------------------------------------------------
You must output ONLY a single JSON object, no extra text or commentary.

Schema:
{
  "pipeline_summary": "short natural-language summary (1â€“3 sentences) of the planned collaboration",
  "steps": [
    {
      "step_index": 1,
      "role": "analyst | researcher | creator | critic | synthesizer",
      "model_id": "provider:model-name",
      "purpose": "high-level description of what this step should accomplish",
      "instructions_for_step": "string with detailed instructions to be used as system/instruction prompt for this step's model",
      "needs_previous_steps": ["analyst", "researcher", ...],
      "estimated_importance": 0.0,
      "model_rationale": "short explanation why this model is chosen for this role"
    }
  ]
}

Details:
- `pipeline_summary`: 1â€“3 sentences maximum. Describe the overall plan at a high level.
- `steps`: Must be ordered in execution order.
- `step_index` starts at 1 and increments by 1 for each step.
- `role` must be one of: "analyst", "researcher", "creator", "critic", "synthesizer".
- `model_id` must match exactly one of the "available_models[i].id".
- `purpose` is a human-readable high-level description.
- `instructions_for_step` is a detailed prompt string your backend will send as system/instruction context to that model.
- `needs_previous_steps` is an array of ROLE NAMES whose artifacts must be provided as context to this step.
- `estimated_importance` is a float 0.0â€“1.0 (how critical this step is to final quality).
- `model_rationale` explains in 1â€“3 sentences why this model is chosen.

You MUST ensure the JSON is valid and parseable. No comments, no trailing commas, no extra keys.

------------------------------------------------------------
INSTRUCTIONS_FOR_STEP CONTENT
------------------------------------------------------------
For each step, `instructions_for_step` should be a clear, standalone instruction block that the model can follow. It should:

1. Define the role: e.g. "You are the Analyst in a multi-model team..."
2. Explain inputs it will receive at runtime: user_message, thread_context, outputs from previous roles
3. Specify EXACTLY what to output, with structure:
   - For analyst: sections like "Goals", "Constraints", "Subtasks", "Success Criteria".
   - For researcher: sections like "Key Facts", "Sources", "Uncertainties / Conflicts".
   - For creator: a complete draft answer in the requested format.
   - For critic: scorecard + bullet list of issues + suggested fixes.
   - For synthesizer: final answer that incorporates critic feedback.

DO NOT include any model selection or routing logic inside `instructions_for_step`.

------------------------------------------------------------
ERROR HANDLING / FALLBACKS
------------------------------------------------------------
If the input JSON is malformed or missing critical fields:
- Produce a minimal plan with a single step:
  - role: "creator"
  - model_id: best overall model from available_models
  - purpose: "Answer the user's question directly."
- Still output valid JSON.

Remember:
You are the planning and routing brain for multi-model collaboration.
You never generate the final user-facing answer yourself.
You only output a JSON plan that will be executed step-by-step."""


class DynamicOrchestrator:
    """
    Orchestrator that uses an LLM to dynamically plan collaboration pipelines.
    """
    
    def __init__(self, orchestrator_model: str = "gpt-4o"):
        self.orchestrator_model = orchestrator_model
        self.system_prompt = ORCHESTRATOR_SYSTEM_PROMPT
    
    async def create_plan(
        self,
        user_message: str,
        api_keys: Dict[str, str],
        thread_context: Optional[str] = None,
        priority: UserPriority = UserPriority.BALANCED,
        max_steps: int = 5
    ) -> CollaborationPlan:
        """
        Create a collaboration plan using the orchestrator LLM.
        
        Args:
            user_message: The user's query
            api_keys: Dict of provider -> API key
            thread_context: Optional previous conversation context
            priority: User's preference for quality/speed/cost
            max_steps: Maximum number of steps to include
            
        Returns:
            CollaborationPlan with steps and model assignments
        """
        start_time = time.perf_counter()
        
        # Get available models based on API keys
        available_models = get_available_models(api_keys)
        
        if not available_models:
            raise ValueError("No AI models available. Please configure at least one API key.")
        
        # Prepare input for orchestrator
        orchestrator_input = {
            "user_message": user_message,
            "thread_context": thread_context,
            "available_models": format_models_for_orchestrator(available_models),
            "user_settings": {
                "priority": priority.value,
                "max_steps": max_steps
            }
        }
        
        # Call orchestrator LLM
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": json.dumps(orchestrator_input, indent=2)}
        ]
        
        # Try to use the orchestrator model
        orchestrator_api_key = api_keys.get("openai")
        
        if orchestrator_api_key:
            response = await call_openai(
                messages=messages,
                model=self.orchestrator_model,
                api_key=orchestrator_api_key,
                temperature=0.3  # Lower temperature for more consistent planning
            )
            plan_json = response.content
        else:
            # Fallback to Gemini if OpenAI not available
            gemini_key = api_keys.get("gemini")
            if gemini_key:
                response = await call_gemini(
                    messages=messages,
                    model="gemini-2.5-flash",
                    api_key=gemini_key
                )
                plan_json = response.content
            else:
                # Create a simple fallback plan without LLM
                return self._create_fallback_plan(available_models, user_message)
        
        # Parse the JSON response
        try:
            # Clean up the response in case it has markdown code blocks
            plan_json = plan_json.strip()
            if plan_json.startswith("```"):
                # Remove markdown code blocks
                lines = plan_json.split("\n")
                plan_json = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])
            
            plan_data = json.loads(plan_json)
        except json.JSONDecodeError as e:
            logger.info("Failed to parse orchestrator response: {e}")
            logger.info("Raw response: {plan_json}")
            return self._create_fallback_plan(available_models, user_message)
        
        # Convert to CollaborationPlan
        steps = []
        for step_data in plan_data.get("steps", []):
            step = CollabStep(
                step_index=step_data.get("step_index", len(steps) + 1),
                role=CollabRole(step_data.get("role", "creator")),
                model_id=step_data.get("model_id", available_models[0].id),
                purpose=step_data.get("purpose", ""),
                instructions_for_step=step_data.get("instructions_for_step", ""),
                needs_previous_steps=step_data.get("needs_previous_steps", []),
                estimated_importance=step_data.get("estimated_importance", 0.5),
                model_rationale=step_data.get("model_rationale", "")
            )
            steps.append(step)
        
        planning_time_ms = (time.perf_counter() - start_time) * 1000
        
        return CollaborationPlan(
            pipeline_summary=plan_data.get("pipeline_summary", ""),
            steps=steps,
            planning_time_ms=planning_time_ms
        )
    
    def _create_fallback_plan(
        self, 
        available_models: List[ModelCapability],
        user_message: str
    ) -> CollaborationPlan:
        """Create a simple fallback plan when orchestrator fails"""
        # Pick the best overall model
        best_model = max(
            available_models,
            key=lambda m: (m.strengths.reasoning + m.strengths.creativity) / 2
        )
        
        return CollaborationPlan(
            pipeline_summary="Direct answer using best available model",
            steps=[
                CollabStep(
                    step_index=1,
                    role=CollabRole.CREATOR,
                    model_id=best_model.id,
                    purpose="Answer the user's question directly",
                    instructions_for_step=f"""You are an AI assistant. Please answer the following question thoroughly and helpfully.

User's question: {user_message}

Provide a comprehensive, well-structured response.""",
                    needs_previous_steps=[],
                    estimated_importance=1.0,
                    model_rationale=f"Using {best_model.display_name} as fallback"
                )
            ],
            planning_time_ms=0.0
        )
    
    async def execute_plan(
        self,
        plan: CollaborationPlan,
        user_message: str,
        api_keys: Dict[str, str],
        turn_id: str,
        thread_context: Optional[str] = None
    ) -> CollaborationResult:
        """
        Execute a collaboration plan step by step.
        
        Args:
            plan: The collaboration plan to execute
            user_message: Original user message
            api_keys: Dict of provider -> API key
            turn_id: Unique identifier for this collaboration run
            thread_context: Optional previous conversation context
            
        Returns:
            CollaborationResult with all step outputs and final answer
        """
        start_time = time.perf_counter()
        step_results: List[StepResult] = []
        step_outputs: Dict[str, str] = {}  # role -> output mapping
        
        for step in plan.steps:
            step_start = time.perf_counter()
            
            try:
                # Build context from previous steps
                context_parts = [f"User Query: {user_message}"]
                
                if thread_context:
                    context_parts.append(f"\nThread Context:\n{thread_context}")
                
                for prev_role in step.needs_previous_steps:
                    if prev_role in step_outputs:
                        context_parts.append(f"\n{prev_role.upper()} Output:\n{step_outputs[prev_role]}")
                
                full_context = "\n".join(context_parts)
                
                # Get model info
                model_info = get_model_by_id(step.model_id)
                if not model_info:
                    raise ValueError(f"Model {step.model_id} not found in registry")
                
                # Get API key for this provider
                provider_key = model_info.provider.value
                api_key = api_keys.get(provider_key)
                
                if not api_key:
                    raise ValueError(f"No API key for provider {provider_key}")
                
                # Build messages
                messages = [
                    {"role": "system", "content": step.instructions_for_step},
                    {"role": "user", "content": full_context}
                ]
                
                # Call the model
                content = await self._call_model(
                    provider=model_info.provider,
                    model=model_info.model_name,
                    messages=messages,
                    api_key=api_key
                )
                
                step_time = (time.perf_counter() - step_start) * 1000
                
                step_results.append(StepResult(
                    step_index=step.step_index,
                    role=step.role.value,
                    model_id=step.model_id,
                    content=content,
                    execution_time_ms=step_time,
                    success=True
                ))
                
                # Store output for subsequent steps
                step_outputs[step.role.value] = content
                
            except Exception as e:
                step_time = (time.perf_counter() - step_start) * 1000
                
                step_results.append(StepResult(
                    step_index=step.step_index,
                    role=step.role.value,
                    model_id=step.model_id,
                    content="",
                    execution_time_ms=step_time,
                    success=False,
                    error=str(e)
                ))
                
                # Continue with remaining steps but log the error
                logger.info("Step {step.step_index} ({step.role}) failed: {e}")
        
        total_time = (time.perf_counter() - start_time) * 1000
        
        # Get final answer (last step's output or best available)
        final_answer = ""
        for result in reversed(step_results):
            if result.success and result.content:
                final_answer = result.content
                break
        
        if not final_answer:
            final_answer = "I apologize, but I was unable to generate a response. Please try again."
        
        return CollaborationResult(
            final_answer=final_answer,
            plan=plan,
            step_results=step_results,
            total_time_ms=total_time,
            turn_id=turn_id
        )
    
    async def _call_model(
        self,
        provider: ProviderType,
        model: str,
        messages: List[Dict[str, str]],
        api_key: str
    ) -> str:
        """Call a model through its provider adapter"""
        
        if provider == ProviderType.OPENAI:
            from app.adapters.openai_adapter import call_openai
            response = await call_openai(
                messages=messages,
                model=model,
                api_key=api_key,
                temperature=0.7
            )
            return response.content
            
        elif provider == ProviderType.GEMINI:
            from app.adapters.gemini import call_gemini
            response = await call_gemini(
                messages=messages,
                model=model,
                api_key=api_key
            )
            return response.content
            
        elif provider == ProviderType.PERPLEXITY:
            from app.adapters.perplexity import call_perplexity
            response = await call_perplexity(
                messages=messages,
                model=model,
                api_key=api_key
            )
            return response.content
            
        elif provider == ProviderType.KIMI:
            from app.adapters.kimi import call_kimi
            response = await call_kimi(
                messages=messages,
                model=model,
                api_key=api_key,
                temperature=0.7
            )
            return response.content
            
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    async def run_collaboration(
        self,
        user_message: str,
        api_keys: Dict[str, str],
        turn_id: str,
        thread_context: Optional[str] = None,
        priority: UserPriority = UserPriority.BALANCED,
        max_steps: int = 5
    ) -> CollaborationResult:
        """
        Run a complete collaboration: plan + execute.
        
        This is the main entry point for running dynamic collaboration.
        
        Args:
            user_message: The user's query
            api_keys: Dict of provider -> API key
            turn_id: Unique identifier for this run
            thread_context: Optional previous conversation context
            priority: User preference (quality/speed/cost/balanced)
            max_steps: Maximum steps to include
            
        Returns:
            Complete CollaborationResult
        """
        # Create the plan
        plan = await self.create_plan(
            user_message=user_message,
            api_keys=api_keys,
            thread_context=thread_context,
            priority=priority,
            max_steps=max_steps
        )
        
        logger.info("ðŸŽ¯ Collaboration Plan: {plan.pipeline_summary}")
        for step in plan.steps:
            logger.info("  Step {step.step_index}: {step.role.value} -> {step.model_id}")
        
        # Execute the plan
        result = await self.execute_plan(
            plan=plan,
            user_message=user_message,
            api_keys=api_keys,
            turn_id=turn_id,
            thread_context=thread_context
        )
        
        return result


# Global orchestrator instance
dynamic_orchestrator = DynamicOrchestrator()






