# Collaborate Pipeline: Complete Implementation Guide

This guide walks you through the complete **Team + Council + Director** collaboration system integrated into your existing chat UI.

## ðŸ“‹ What You Have

### Backend
- âœ… **Pipeline orchestrator** (`app/services/collaborate/pipeline.py`) â€” runs 5-stage inner team + council + director
- âœ… **Streaming support** (`app/services/collaborate/streaming.py`) â€” emits real-time events for animation
- âœ… **System prompts** (3 files) â€” ready to customize
- âœ… **Endpoints**:
  - `POST /threads/{thread_id}/collaborate` â€” one-shot response
  - `POST /threads/{thread_id}/collaborate-stream` â€” SSE stream with thinking animation

### Frontend
- âœ… **Event types** (`types/collaborate-events.ts`) â€” TypeScript definitions
- âœ… **ThinkingStrip component** (`components/collaborate/ThinkingStrip.tsx`) â€” animated thinking UI
- âœ… **useThinkingState hook** (`hooks/useThinkingState.ts`) â€” state management
- âœ… **Example chat component** (`components/collaborate/ChatWithThinking.example.tsx`) â€” how to wire it all together

### Testing & Observability
- âœ… **Test script** (`test_collaborate.py`) â€” validate pipeline end-to-end
- âœ… **Logging & monitoring** (`app/services/collaborate/observability.py`) â€” structured logs + OpenTelemetry

---

## ðŸš€ Getting Started (Today)

### Step 1: Test the Pipeline

Run the test script to verify everything works:

```bash
cd /Users/rao305/Documents/DAC/backend
python test_collaborate.py
```

This will:
1. Send 4 test prompts to the collaborate endpoint
2. Verify responses have the right structure
3. Show timing/latency metrics
4. Save results to `test_results.json`

**Expected output:**
```
âœ… PASS: Simple factual
âœ… PASS: Multi-part reasoning
âœ… PASS: Ambiguous
âœ… PASS: Long instruction

Total: 4/4 passed
```

If tests fail, check:
- [ ] FastAPI server is running (`python main.py` in backend/)
- [ ] Thread `test-thread-1` exists in DB
- [ ] Org ID header is set correctly (`x-org-id: org_demo`)
- [ ] All provider API keys are configured (OpenAI, Gemini, Perplexity, Kimi, OpenRouter)

### Step 2: Wire into Your Chat UI

Copy the pattern from `ChatWithThinking.example.tsx`:

```tsx
import { ChatWithThinking } from "@/components/collaborate/ChatWithThinking.example";

export default function ChatPage({ threadId }: { threadId: string }) {
  return <ChatWithThinking threadId={threadId} />;
}
```

The component:
- Shows `ThinkingStrip` while the pipeline runs
- Streams thinking events in real-time
- Collapses when complete
- Shows final answer with confidence badge

### Step 3: Customize Models (Optional)

Currently hardcoded in `threads.py`:

```python
inner_models = {
    "analyst": ModelInfo(provider="google", model_slug="gemini-2.0-pro-exp-02-05", ...),
    "researcher": ModelInfo(provider="perplexity", model_slug="sonar-reasoning", ...),
    # ... etc
}
```

To change models:
1. Edit `collaborate_thread()` function in `/app/api/threads.py` (lines ~2055â€“2120)
2. Or load from org settings database (add TODO marker)
3. Restart FastAPI server

---

## ðŸ§  Understanding the Flow

### What Happens When User Clicks "Collaborate"

**Backend:**
1. **Analyst** reads question, breaks into subquestions
2. **Researcher** gathers facts and structured notes
3. **Creator** writes first draft from analysis + research
4. **Critic** reviews draft, points out issues
5. **Internal Synth** combines into high-quality report
6. **Compression** summarizes report (300 tokens)
7. **Council** (5 models) review in parallel:
   - Perplexity, Gemini, GPT, Kimi, OpenRouter
   - Each votes: agree / mixed / disagree
8. **Director** synthesizes final answer using internal report + council feedback

**Frontend:**
- SSE events stream in real-time
- `ThinkingStrip` animates each step
- Shows council progress (3/5 reviews, stance counts)
- Streams final answer character-by-character
- Collapses when done

**Output:**
User sees one clean answer with:
- Confidence badge (high/medium/low)
- "This was generated by 7-step collaboration"
- Optional "View detailed reasoning" button

---

## ðŸ”§ Customization

### 1. Tweak System Prompts

Prompts live in `/backend/app/services/collaborate/prompts/`:

- `inner_team_system.txt` â€” controls all 5 inner stages
- `council_reviewer_system.txt` â€” controls how council critiques
- `director_system.txt` â€” controls final synthesis

**To iterate:**
1. Edit the `.txt` file
2. Add version comment at top: `# version: v1.0.1 - 2025-11-30`
3. Restart server (prompts are loaded at request time)
4. Re-run test script to see changes

**Common tweaks:**
- "Critic, focus more on accuracy than style" â†’ tell Critic to check facts
- "Council, be generous about 'agree'" â†’ reduce false disagreements
- "Director, flag uncertainties" â†’ make output more transparent

### 2. Change Role Model Assignments

Edit `inner_models` dict in `threads.py` to swap models:

```python
# Before
"analyst": ModelInfo(provider="google", model_slug="gemini-2.0-pro-exp-02-05", ...),

# After (use faster model for analyst)
"analyst": ModelInfo(provider="openai", model_slug="gpt-4o-mini", ...),
```

### 3. Add Observability Hooks

The `CollaborateObserver` class tracks everything:

```python
from app.services.collaborate.observability import CollaborateObserver

observer = CollaborateObserver(run_id="...", org_id="...", thread_id="...")
observer.log_stage_start("analyst", "gemini-2.0-pro")
observer.log_stage_end("analyst", latency_ms=1200, tokens_in=500, tokens_out=300)
# ... etc
observer.log_completion(response)  # logs full summary
```

This emits:
- JSON structured logs (queryable)
- OpenTelemetry spans (if enabled)

Logs go to stdout by default, route to your logging service.

---

## ðŸ“Š Monitoring & Debugging

### View Raw Collaborate Response

Hit the non-streaming endpoint:

```bash
curl -X POST "http://localhost:8000/threads/test-thread-1/collaborate" \
  -H "x-org-id: org_demo" \
  -H "Content-Type: application/json" \
  -d '{"message":"Who is Andrej Karpathy?","mode":"auto"}' | jq .
```

Response includes:
- `final_answer` â€” user-facing text
- `internal_pipeline` â€” 5 stages + their outputs
- `external_reviews` â€” council critiques (issues, missing points, suggestions)
- `meta` â€” run ID, latency, models involved

### Debug a Specific Stage

If a stage fails, check:
1. **API keys** â€” does the provider have auth?
2. **Model name** â€” is `model_slug` correct for the provider?
3. **Prompt** â€” is the system prompt valid?
4. **Input** â€” is `collab_state` being passed correctly?

Add logging in `pipeline.py`:

```python
print(f"DEBUG: {role} received state keys: {collab_state.model_dump().keys()}")
```

### Analyze Performance

Extract metrics:

```python
from app.services.collaborate.observability import emit_performance_metrics

metrics = emit_performance_metrics(response)
print(f"Total: {metrics['total_latency_ms']}ms")
for role, data in metrics['stages'].items():
    print(f"  {role}: {data['latency_ms']}ms ({data['model']})")
```

Typical timings:
- Analyst: 0.5â€“1.0s (quick analysis)
- Researcher: 1.5â€“3.0s (fact gathering)
- Creator: 1.0â€“2.0s (draft writing)
- Critic: 0.8â€“1.5s (review)
- Internal Synth: 1.0â€“2.0s (final synthesis)
- Council (parallel): 2.0â€“4.0s (5 reviewers)
- Director: 1.5â€“3.0s (final answer)

**Total:** ~9â€“15 seconds typical.

To speed up:
- Use faster models (gpt-4o-mini instead of gpt-4o)
- Reduce council size (3 instead of 5 reviewers)
- Shorten internal report before compression

---

## ðŸš¨ Troubleshooting

| Issue | Solution |
|-------|----------|
| `No API keys configured` | Check `app/services/provider_keys.py`, ensure org has API keys for all providers |
| Stage timeout | Increase max context in `DEFAULT_COMPLETION_TOKENS` |
| Council not running | Check if API keys are present for each council provider |
| Final answer is generic | Adjust Director system prompt to be more specific |
| Thinking UI doesn't animate | Check browser console for SSE connection errors |
| Responses are duplicative | Critic is not catching issues â€” adjust Critic system prompt |

---

## ðŸ“š File Reference

### Backend
```
app/services/collaborate/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models.py                 # Pydantic schemas
â”œâ”€â”€ pipeline.py              # Core orchestrator
â”œâ”€â”€ streaming.py             # SSE event streaming
â”œâ”€â”€ observability.py         # Logging + monitoring
â””â”€â”€ prompts/
    â”œâ”€â”€ inner_team_system.txt
    â”œâ”€â”€ council_reviewer_system.txt
    â””â”€â”€ director_system.txt

app/api/threads.py           # Two new endpoints: /collaborate, /collaborate-stream
```

### Frontend
```
types/
â””â”€â”€ collaborate-events.ts      # Event type definitions

components/collaborate/
â”œâ”€â”€ ThinkingStrip.tsx         # Main thinking animation component
â””â”€â”€ ChatWithThinking.example.tsx  # How to integrate into chat

hooks/
â””â”€â”€ useThinkingState.ts        # State management hook
```

### Testing
```
backend/
â””â”€â”€ test_collaborate.py        # Test script (run with: python test_collaborate.py)
```

---

## âœ… Checklist for Ship

- [ ] All provider API keys are configured
- [ ] Test script passes (4/4)
- [ ] Chat component displays thinking animation
- [ ] Final answer renders correctly
- [ ] Collapse/expand button works
- [ ] Logging is captured (check logs/)
- [ ] Performance is acceptable (<20s end-to-end)
- [ ] Error handling doesn't crash UI
- [ ] Confidence badges display correctly
- [ ] System prompts are customized for your domain

---

## ðŸŽ¯ Next Phase Ideas

Once basic implementation is solid:

1. **A/B Test Prompts** â€” route 50% users to v1, 50% to v2, compare outputs
2. **Custom Model Selection** â€” per-user or per-question model picking
3. **Feedback Loop** â€” users rate thinking steps, feed back to improve prompts
4. **Advanced Metrics** â€” track which council reviewers are most useful
5. **Caching** â€” reuse internal reports for similar questions
6. **Streaming Internal Report** â€” show internal_synth work as it happens

---

## ðŸ“ž Support

Check these files for examples:
- `frontend/components/collaborate/ChatWithThinking.example.tsx` â€” complete example
- `backend/test_collaborate.py` â€” test patterns
- `backend/app/services/collaborate/observability.py` â€” logging examples

Questions? Check the prompts â€” they're the main tuning surface.
